{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the path to the JSON file\n",
    "file_path = './dataset/train.json'\n",
    "\n",
    "# Read the JSON file into a DataFrame\n",
    "df = pd.read_json(file_path)\n",
    "\n",
    "# Preview the first few rows of the DataFrame\n",
    "df.transpose()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "def preprocess_dataset_for_bert(df):\n",
    "    # Concatenate 'Title' and 'Body' columns to add context\n",
    "    df['Context'] = df['Title'] + ' ' + df['Body']\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    df['Comment'] = df['Comment'].apply(lambda x: ' '.join([word for word in word_tokenize(x) if word.lower() not in stop_words]))\n",
    "    \n",
    "    # Tokenize text\n",
    "    df['Context'] = df['Context'].apply(word_tokenize)\n",
    "    df['Comment'] = df['Comment'].apply(word_tokenize)\n",
    "    \n",
    "    # Lemmatize text\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    df['Context'] = df['Context'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "    df['Comment'] = df['Comment'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "    \n",
    "    # Perform spell checking and correction\n",
    "    spell = SpellChecker()\n",
    "    df['Context'] = df['Context'].apply(lambda x: [spell.correction(word) for word in x])\n",
    "    df['Comment'] = df['Comment'].apply(lambda x: [spell.correction(word) for word in x])\n",
    "    \n",
    "    # Convert tokenized text back to string\n",
    "    df['Context'] = df['Context'].apply(' '.join)\n",
    "    df['Comment'] = df['Comment'].apply(' '.join)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def train_bert_model(df):\n",
    "    # Load pre-trained BERT tokenizer and model\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    # Encode the 'Context' and 'Comment' columns using BERT tokenizer\n",
    "    encoded_data = tokenizer.batch_encode_plus(df[['Context', 'Comment']].values,\n",
    "                                                add_special_tokens=True,\n",
    "                                                return_attention_mask=True,\n",
    "                                                return_tensors='pt')\n",
    "    \n",
    "    # Split the dataset into train and test sets\n",
    "    input_ids = encoded_data['input_ids']\n",
    "    attention_masks = encoded_data['attention_mask']\n",
    "    labels = df['L1: Type'].values\n",
    "    train_inputs, test_inputs, train_labels, test_labels = train_test_split(input_ids,\n",
    "                                                                            labels,\n",
    "                                                                            test_size=0.2,\n",
    "                                                                            random_state=42)\n",
    "    train_masks, test_masks, _, _ = train_test_split(attention_masks,\n",
    "                                                     input_ids,\n",
    "                                                     test_size=0.2,\n",
    "                                                     random_state=42)\n",
    "    \n",
    "    # Convert data to PyTorch tensors\n",
    "    train_inputs = train_inputs.to('cuda')\n",
    "    train_labels = torch.tensor(train_labels).to('cuda')\n",
    "    train_masks = train_masks.to('cuda')\n",
    "    \n",
    "    test_inputs = test_inputs.to('cuda')\n",
    "    test_labels = torch.tensor(test_labels).to('cuda')\n",
    "    test_masks = test_masks.to('cuda')\n",
    "    \n",
    "    # Fine-tune BERT model on training data\n",
    "    model.to('cuda')\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "    epochs = 3\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(train_inputs,\n",
    "                        token_type_ids=None,\n",
    "                        attention_mask=train_masks,\n",
    "                        labels=train_labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Evaluate BERT model on test data\n",
    "    model.eval()\n",
    "    test_outputs = model(test_inputs,\n",
    "                          token_type_ids=None,\n",
    "                          attention_mask=test_masks,\n",
    "                          labels=test_labels)\n",
    "    _, preds = torch.max(test_outputs.logits, dim=1)\n",
    "    preds = preds.cpu().numpy()\n",
    "    test_labels = test_labels.cpu().numpy()\n",
    "    report = classification_report(test_labels, preds)\n",
    "    \n",
    "    return model, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed dataset into a DataFrame (df)\n",
    "\n",
    "# Perform necessary data preprocessing steps (text normalization, stopword removal, tokenization, etc.) on the dataset\n",
    "\n",
    "# Call the train_bert_model() function\n",
    "trained_model, classification_report = train_bert_model(df)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\\n\", classification_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AlbertTokenizer, AlbertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def train_albert_model(df):\n",
    "    # Load pre-trained ALBERT tokenizer and model\n",
    "    tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
    "    model = AlbertForSequenceClassification.from_pretrained('albert-base-v2')\n",
    "    \n",
    "    # Encode the 'Context' and 'Comment' columns using ALBERT tokenizer\n",
    "    encoded_data = tokenizer.batch_encode_plus(df[['Context', 'Comment']].values,\n",
    "                                                add_special_tokens=True,\n",
    "                                                return_attention_mask=True,\n",
    "                                                return_tensors='pt')\n",
    "    \n",
    "    # Split the dataset into train and test sets\n",
    "    input_ids = encoded_data['input_ids']\n",
    "    attention_masks = encoded_data['attention_mask']\n",
    "    labels = df['L1: Type'].values\n",
    "    train_inputs, test_inputs, train_labels, test_labels = train_test_split(input_ids,\n",
    "                                                                            labels,\n",
    "                                                                            test_size=0.2,\n",
    "                                                                            random_state=42)\n",
    "    train_masks, test_masks, _, _ = train_test_split(attention_masks,\n",
    "                                                     input_ids,\n",
    "                                                     test_size=0.2,\n",
    "                                                     random_state=42)\n",
    "    \n",
    "    # Convert data to PyTorch tensors\n",
    "    train_inputs = train_inputs.to('cuda')\n",
    "    train_labels = torch.tensor(train_labels).to('cuda')\n",
    "    train_masks = train_masks.to('cuda')\n",
    "    \n",
    "    test_inputs = test_inputs.to('cuda')\n",
    "    test_labels = torch.tensor(test_labels).to('cuda')\n",
    "    test_masks = test_masks.to('cuda')\n",
    "    \n",
    "    # Fine-tune ALBERT model on training data\n",
    "    model.to('cuda')\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "    epochs = 3\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(train_inputs,\n",
    "                        token_type_ids=None,\n",
    "                        attention_mask=train_masks,\n",
    "                        labels=train_labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Evaluate ALBERT model on test data\n",
    "    model.eval()\n",
    "    test_outputs = model(test_inputs,\n",
    "                          token_type_ids=None,\n",
    "                          attention_mask=test_masks,\n",
    "                          labels=test_labels)\n",
    "    _, preds = torch.max(test_outputs.logits, dim=1)\n",
    "    preds = preds.cpu().numpy()\n",
    "    test_labels = test_labels.cpu().numpy()\n",
    "    report = classification_report(test_labels, preds)\n",
    "    \n",
    "    return model, report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed dataset into a DataFrame (df)\n",
    "\n",
    "# Perform necessary data preprocessing steps (text normalization, stopword removal, tokenization, etc.) on the dataset\n",
    "\n",
    "# Call the train_bert_model() function\n",
    "trained_model, classification_report = train_albert_model(df)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\\n\", classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def train_roberta_model(df):\n",
    "    # Load pre-trained RoBERTa tokenizer and model\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    model = RobertaForSequenceClassification.from_pretrained('roberta-base')\n",
    "    \n",
    "    # Encode the 'Context' and 'Comment' columns using RoBERTa tokenizer\n",
    "    encoded_data = tokenizer.batch_encode_plus(df[['Context', 'Comment']].values,\n",
    "                                                add_special_tokens=True,\n",
    "                                                return_attention_mask=True,\n",
    "                                                return_tensors='pt')\n",
    "    \n",
    "    # Split the dataset into train and test sets\n",
    "    input_ids = encoded_data['input_ids']\n",
    "    attention_masks = encoded_data['attention_mask']\n",
    "    labels = df['L1: Type'].values\n",
    "    train_inputs, test_inputs, train_labels, test_labels = train_test_split(input_ids,\n",
    "                                                                            labels,\n",
    "                                                                            test_size=0.2,\n",
    "                                                                            random_state=42)\n",
    "    train_masks, test_masks, _, _ = train_test_split(attention_masks,\n",
    "                                                     input_ids,\n",
    "                                                     test_size=0.2,\n",
    "                                                     random_state=42)\n",
    "    \n",
    "    # Convert data to PyTorch tensors\n",
    "    train_inputs = train_inputs.to('cuda')\n",
    "    train_labels = torch.tensor(train_labels).to('cuda')\n",
    "    train_masks = train_masks.to('cuda')\n",
    "    \n",
    "    test_inputs = test_inputs.to('cuda')\n",
    "    test_labels = torch.tensor(test_labels).to('cuda')\n",
    "    test_masks = test_masks.to('cuda')\n",
    "    \n",
    "    # Fine-tune RoBERTa model on training data\n",
    "    model.to('cuda')\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "    epochs = 3\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(train_inputs,\n",
    "                        token_type_ids=None,\n",
    "                        attention_mask=train_masks,\n",
    "                        labels=train_labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Evaluate RoBERTa model on test data\n",
    "    model.eval()\n",
    "    test_outputs = model(test_inputs,\n",
    "                          token_type_ids=None,\n",
    "                          attention_mask=test_masks,\n",
    "                          labels=test_labels)\n",
    "    _, preds = torch.max(test_outputs.logits, dim=1)\n",
    "    preds = preds.cpu().numpy()\n",
    "    test_labels = test_labels.cpu().numpy()\n",
    "    report = classification_report(test_labels, preds)\n",
    "    \n",
    "    return model, report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed dataset into a DataFrame (df)\n",
    "\n",
    "# Perform necessary data preprocessing steps (text normalization, stopword removal, tokenization, etc.) on the dataset\n",
    "\n",
    "# Call the train_bert_model() function\n",
    "trained_model, classification_report = train_roberta_model(df)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\\n\", classification_report)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
